{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e1a6de-b224-4fed-a8ba-8aa30e049c08",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a201e64-f588-460d-8ab4-1e20b6f8bf3b",
   "metadata": {},
   "source": [
    "In this notebook, we describe the process of data collection for our project. Due to limited access to the Twitter API, we utilized an alternative method using [ScraperAPI](https://www.scraperapi.com/twitter-scraper/), a popular web scraping tool founded in 2018. ScraperAPI claims that it is legal to scrape Twitter without its API as most Twitter data is publicly accessible, which makes it legal to extract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6388610-7fe6-40ee-95c5-34ac21ef77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3e5386-f796-408f-ad6f-743fd3c5a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbca281-aee3-4a97-8ce0-37d9b5b47a7b",
   "metadata": {},
   "source": [
    "### Scrape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21539d59-25d7-4ecd-8d3a-e59a5c6e2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store data in json from scraping\n",
    "folder_path = \"raw_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c712ce7-9c50-47ad-80c9-c7fd037f6ee4",
   "metadata": {},
   "source": [
    "The `scrape_data` function below is designed to retrieve data using an API and store the results in a JSON file. Due to limitations on the number of API calls, the function performs multiple scrapes for a company. The function makes 15 consecutive requests, each time retrieving a batch of approximately 19 tweets. The responses are then exported to a json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdb75ea-5ddd-43b7-a084-ecbd728e03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(file_name, next_cursor, user_id):\n",
    "    payload = {'api_key': api_key, 'user_id': user_id, 'next_cursor': next_cursor}\n",
    "\n",
    "    responses = []\n",
    "    # 15 consecutive requests\n",
    "    for _ in range(15):\n",
    "        r = requests.get('https://api.scraperapi.com/structured/twitter/v2/tweets', params=payload)\n",
    "        data = r.json()\n",
    "        responses.append(data)\n",
    "\n",
    "        # update the new next_cursor value\n",
    "        payload['next_cursor'] = data['next_cursor']\n",
    "\n",
    "    # a dictionary to store the name and responses\n",
    "    export_data = {\n",
    "        'name': file_name,\n",
    "        'responses': responses\n",
    "    }\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # export the data to JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(export_data, file)\n",
    "\n",
    "    print(f\"Scraping completed and responses exported to '{file_name}' file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93c7fa86-13ab-4e79-88bc-cd24a43bbb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentive\n",
    "# scrape_data(\"momentive-5.json\", \"HBaQwLDh99yXyCcAAA==\", \"1384685957216092161\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4350ba-0bbc-4ff0-92a0-32116fa55c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wazoku\n",
    "# scrape_data(\"wazoku-3.json\",\"HBaEgL3NzbiwgScAAA==\",\"304002619\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95312e0d-ec1c-4326-ba4c-63c07a4c2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPART\n",
    "# scrape_data(\"inpart-4.json\",\"HBb+v7Llv4G80CYAAA==\",\"819883602\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fa4cd9c-160c-4809-bcc6-0f7c84b9ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ninesigma\n",
    "# scrape_data(\"ninesigma-3.json\",\"HBaCwLXl9pjenB0AAA==\",\"19405388\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a368cee-b598-496e-b86e-18a127185a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yet2com\n",
    "# scrape_data(\"yet2com-4.json\",\"HBaKwL35gaSg1B8AAA==\",\"63966920\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c8d2a4b-ec2c-49ba-b2ae-e65fbb1dfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# innoget\n",
    "# scrape_data(\"innoget-1.json\",\"NEXT_CURSOR_VALUE\",\"72579390\")\n",
    "# scrape_data(\"innoget-2.json\",\"HBaAgLaNj/jg0hMAAA==\",\"72579390\")\n",
    "# scrape_data(\"innoget-3.json\",\"HBaAwLe5y+eHrRAAAA==\",\"72579390\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e9356-e298-46b4-a37e-6dc6cbcc18ca",
   "metadata": {},
   "source": [
    "### Combine to single DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758fa43-ade8-440a-ba22-61308fff80c3",
   "metadata": {},
   "source": [
    "After getting all tweets from competitors, we extract the desired fields, and combine them into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f2e452-1098-4835-bc3a-4eaa293b4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    print(filename)\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        response_json = json.load(file)\n",
    "    \n",
    "    # extract the desired fields from the JSON data\n",
    "    tweet_data = []\n",
    "    skipped_count = 0  # Counter for skipped rows\n",
    "    for item in response_json['responses']:\n",
    "        if 'tweets' in item:\n",
    "            tweets = item['tweets']\n",
    "            for tweet in tweets:\n",
    "                try:\n",
    "                    user_id = tweet['user_id']\n",
    "                    user_name = tweet['user_name']\n",
    "                    date = tweet['date']\n",
    "                    tweet_id = tweet['tweet_id']\n",
    "                    text = tweet['text']\n",
    "                    is_reply = tweet['is_reply']\n",
    "                    replies = tweet['replies']\n",
    "                    retweets = tweet['retweets']\n",
    "                    quotes = tweet['quotes']\n",
    "                \n",
    "                    # append the extracted data to the list\n",
    "                    tweet_data.append({\n",
    "                        'user_id': user_id,\n",
    "                        'user_name': user_name,\n",
    "                        'date': date,\n",
    "                        'text': text,\n",
    "                        'tweet_id': tweet_id,\n",
    "                        'is_reply': is_reply,\n",
    "                        'replies': replies,\n",
    "                        'retweets': retweets,\n",
    "                        'quotes': quotes\n",
    "                    })\n",
    "                except KeyError:\n",
    "                    # Increment the skipped_count if user_id or user_name is not found\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "    \n",
    "    df = pd.DataFrame(tweet_data)\n",
    "    print(f\"Skipped rows: {skipped_count}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f710578b-9e66-4790-b791-392709207633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fucntion to check the duplicated data\n",
    "def check_dup(df):\n",
    "    duplicated_tweet_ids = df[df.duplicated('tweet_id', keep=False)]\n",
    "\n",
    "    if not duplicated_tweet_ids.empty:\n",
    "        print(\"Duplicated tweet_id values found:\")\n",
    "        print(duplicated_tweet_ids)\n",
    "    else:\n",
    "        print(\"No duplicated tweet_id values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8949c42d-f5f2-44bf-a9b7-09069413fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all json scrapping data \n",
    "raw_data = [f for f in os.listdir(\"raw_data/\") if f.endswith(\".json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a571927-eea3-4ae4-869d-4d30ea7c6fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['innoget-2.json',\n",
       " 'momentive-5.json',\n",
       " 'inpart-1.json',\n",
       " 'ninesigma-2.json',\n",
       " 'ninesigma-3.json',\n",
       " 'momentive-4.json',\n",
       " 'innoget-3.json',\n",
       " 'wazoku-1.json',\n",
       " 'momentive-3.json',\n",
       " 'momentive-2.json',\n",
       " 'yet2com-1.json',\n",
       " 'momentive-1.json',\n",
       " 'yet2com-2.json',\n",
       " 'yet2com-3.json',\n",
       " 'inpart-4.json',\n",
       " 'wazoku-2.json',\n",
       " 'inpart-3.json',\n",
       " 'yet2com-4.json',\n",
       " 'inpart-2.json',\n",
       " 'ninesigma-1.json',\n",
       " 'wazoku-3.json',\n",
       " 'innoget-1.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738c2616-68f0-4b2d-8809-39e5990f29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df to store combined data\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edadeec5-d9c4-4606-8f1f-19668e4479e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "innoget-2.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "momentive-5.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "inpart-1.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "ninesigma-2.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "ninesigma-3.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "momentive-4.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "innoget-3.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "wazoku-1.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "momentive-3.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "momentive-2.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "yet2com-1.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "momentive-1.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "yet2com-2.json\n",
      "Skipped rows: 1\n",
      "No duplicated tweet_id values found.\n",
      "yet2com-3.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "inpart-4.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "wazoku-2.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "inpart-3.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "yet2com-4.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "inpart-2.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "ninesigma-1.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "wazoku-3.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n",
      "innoget-1.json\n",
      "Skipped rows: 0\n",
      "No duplicated tweet_id values found.\n"
     ]
    }
   ],
   "source": [
    "for file in raw_data:\n",
    "    temp_df = load_data(file)\n",
    "    df = pd.concat([df,temp_df], ignore_index=True, sort=False)\n",
    "    check_dup(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd23a4fc-426d-4ba7-ae7a-3903251c076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72579390</td>\n",
       "      <td>innoget</td>\n",
       "      <td>Mon Feb 29 11:23:20 +0000 2016</td>\n",
       "      <td>RT @PRUAB: 3,2M€ #GrantCall per a projectes #c...</td>\n",
       "      <td>704265693152337921</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72579390</td>\n",
       "      <td>innoget</td>\n",
       "      <td>Fri Feb 26 11:35:21 +0000 2016</td>\n",
       "      <td>#Global #Biotech Reagents Market 2016 Industry...</td>\n",
       "      <td>703181551727570944</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72579390</td>\n",
       "      <td>innoget</td>\n",
       "      <td>Thu Feb 25 10:15:16 +0000 2016</td>\n",
       "      <td>Tech Transfer Office in #Ohio #University help...</td>\n",
       "      <td>702799011187658752</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72579390</td>\n",
       "      <td>innoget</td>\n",
       "      <td>Wed Feb 24 12:10:05 +0000 2016</td>\n",
       "      <td>What’s your point regarding IP protection? Doe...</td>\n",
       "      <td>702465520109559808</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72579390</td>\n",
       "      <td>innoget</td>\n",
       "      <td>Tue Feb 23 15:15:17 +0000 2016</td>\n",
       "      <td>New article about #Samsung and its investment ...</td>\n",
       "      <td>702149739492597761</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id user_name                            date  \\\n",
       "0  72579390   innoget  Mon Feb 29 11:23:20 +0000 2016   \n",
       "1  72579390   innoget  Fri Feb 26 11:35:21 +0000 2016   \n",
       "2  72579390   innoget  Thu Feb 25 10:15:16 +0000 2016   \n",
       "3  72579390   innoget  Wed Feb 24 12:10:05 +0000 2016   \n",
       "4  72579390   innoget  Tue Feb 23 15:15:17 +0000 2016   \n",
       "\n",
       "                                                text            tweet_id  \\\n",
       "0  RT @PRUAB: 3,2M€ #GrantCall per a projectes #c...  704265693152337921   \n",
       "1  #Global #Biotech Reagents Market 2016 Industry...  703181551727570944   \n",
       "2  Tech Transfer Office in #Ohio #University help...  702799011187658752   \n",
       "3  What’s your point regarding IP protection? Doe...  702465520109559808   \n",
       "4  New article about #Samsung and its investment ...  702149739492597761   \n",
       "\n",
       "   is_reply  replies  retweets  quotes  \n",
       "0     False        0         1       0  \n",
       "1     False        0         0       0  \n",
       "2     False        0         0       0  \n",
       "3     False        0         0       0  \n",
       "4     False        0         0       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a4fb781-989c-4bfb-9800-a00c76a2af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_name\n",
       "yet2com        849\n",
       "MomentiveAI    846\n",
       "IN_PART        840\n",
       "NineSigma      832\n",
       "WazokuHq       819\n",
       "innoget        798\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('user_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3812b0-77e6-4721-b8fd-569918406391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4984 entries, 0 to 4983\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   user_id    4984 non-null   object\n",
      " 1   user_name  4984 non-null   object\n",
      " 2   date       4984 non-null   object\n",
      " 3   text       4984 non-null   object\n",
      " 4   tweet_id   4984 non-null   object\n",
      " 5   is_reply   4984 non-null   bool  \n",
      " 6   replies    4984 non-null   int64 \n",
      " 7   retweets   4984 non-null   int64 \n",
      " 8   quotes     4984 non-null   int64 \n",
      "dtypes: bool(1), int64(3), object(5)\n",
      "memory usage: 316.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6346cc2a-5812-404d-8517-f2f3156b5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# export to pickle\n",
    "df.to_pickle(\"pickle_files/all_tweets.pkl\")\n",
    "print(\"DataFrames exported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be5e7458-6baa-44a0-a57b-c05ac6fbaf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# export to CSV\n",
    "df.to_csv(\"csv_files/all_tweets.csv\", index=False)\n",
    "print(\"DataFrames exported successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
